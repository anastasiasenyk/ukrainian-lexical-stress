{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec842859-e53d-421b-a532-9b85d19ccc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: https://github.com/M-Taghizadeh/flan-t5-base-imdb-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60569bc-2b78-402d-8c5a-4f65059f31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers pytorch_lightning sentencepiece datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84cc2d-6259-45f9-8fd1-65a50f5e953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tqdm pandas scikit-learn evaluate nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529bb1f-8604-4013-8f0e-8f94afb4b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c57ddb9c-f41e-4a51-8ecf-34f915aac68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"hf_blabla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fe9dff-ccd1-4b01-8b15-96a396a00534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96f94ee-c1bb-4d7d-a5fa-84606a293088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "UKRAINIAN_LETTERS = 'абвгґдеєжзиіїйклмнопрстуфхцчшщьюя'\n",
    "UKRAINAIN_VOWELS = 'аеєиіїоуюя'\n",
    "ENGLISH_LETTERS = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d71318-1fcc-4b7e-be03-657bd884c16f",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9b21dd7-2117-44ff-89ad-65debfd4c5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137078, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_punctuation = \"\"\" .,!?;:'\"«»()+-—–\"\"\"\n",
    "other_punctuation =  \"\"\" $%&<>{}[]*\"\"\"\n",
    "\n",
    "voa_df = pd.read_csv('./voa_stressed_cleaned_data.csv')\n",
    "unique_letters = set(''.join(voa_df['text'].to_list()))\n",
    "\n",
    "unique_letters = unique_letters - set(UKRAINIAN_LETTERS) \\\n",
    "                                - set(UKRAINIAN_LETTERS.upper()) \\\n",
    "                                - set(allowed_punctuation) \\\n",
    "                                - set(other_punctuation)\n",
    "\n",
    "df = voa_df[~voa_df['text'].apply(lambda x: any(c in unique_letters for c in x))]\n",
    "df = df[['text', 'labels']]\n",
    "df = df.rename(columns={\n",
    "    'labels': 'label'\n",
    "})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c2c3b76-2981-494a-88a7-80d8388ff448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    train_df, eval_df = train_test_split(df, test_size=0.01, random_state=42)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df).remove_columns(['__index_level_0__'])\n",
    "    eval_dataset = Dataset.from_pandas(eval_df).remove_columns(['__index_level_0__'])\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"eval\": eval_dataset\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "\n",
    "model_id = 'google/byt5-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "dataset = get_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02f4421-69fa-4ea8-84ce-05c6942ec7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db242a63a76b41b684a1dc56bab1189d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/137078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69e81520c804e85b0011272421ae339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/137078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]]).map(lambda x: tokenizer(x[\"text\"], truncation=True), batched=True, remove_columns=['text', 'label'])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "\n",
    "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"eval\"]]).map(lambda x: tokenizer(x[\"label\"], truncation=True), batched=True, remove_columns=['text', 'label'])\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd78441d-e842-4907-b1f1-650bc5ddf237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1364, 1467)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_source_length, max_target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ba957ce-604b-45ec-803d-360d0af5a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    # print(sample)\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [item for item in sample[\"text\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=sample[\"label\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"label\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e05492-2eb9-422d-93fc-558a9039b6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46847a44f9044e9896358dc0268a31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/135707 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206204c1c7ff41888bfbc543b1db3859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1123370b-e3b2-49fd-a48c-8fb1d766def8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67aff46564374c5d84a04ecb16864966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/135707 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b98ed20ae74464c8b6c681f88487863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset.save_to_disk(\"tokenized_voa_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6e546-409b-43a4-9093-8519a763da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk(\"tokenized_voa_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52066c-4f18-4d8b-b639-996ec7b757d1",
   "metadata": {},
   "source": [
    "# Setup metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c0f34442-4f19-4e1d-b0c8-1f4b81fecec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "UKRAINAIN_VOWELS = 'аеєиіїоуюя'\n",
    "\n",
    "def compute_stress_metrics(eval_preds):\n",
    "    global UKRAINAIN_VOWELS, tokenizer\n",
    "    \n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    def split_with_stress(sentence):\n",
    "        return re.split(r'(\\s+|-)', sentence)\n",
    "\n",
    "    match_results = []\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        pred_split = split_with_stress(pred)\n",
    "        label_split = split_with_stress(label)\n",
    "\n",
    "        if len(pred_split) == len(label_split):\n",
    "            matches = []\n",
    "            for p, l in zip(pred_split, label_split):\n",
    "                if not any(char in UKRAINAIN_VOWELS for char in l):\n",
    "                    continue\n",
    "                if p==l:\n",
    "                    matches.append(1)\n",
    "                elif p.replace('+', '') != l.replace('+', ''):\n",
    "                    matches = [0]\n",
    "                    break\n",
    "                else:\n",
    "                    matches.append(0)\n",
    "            if not matches: matches=[0]\n",
    "            match_results.append(np.mean(matches))\n",
    "        else:\n",
    "            match_results.append(0) \n",
    "\n",
    "    # average match score across\n",
    "    result = {\"average_match\": np.mean(match_results) * 100}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "44d2db02-f20d-41e5-9f74-1d9eac194ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_match': 100.0}\n",
      "{'average_match': 66.66666666666666}\n",
      "{'average_match': 33.33333333333333}\n",
      "{'average_match': 0.0}\n",
      "{'average_match': 0.0}\n",
      "{'average_match': 0.0}\n",
      "Average: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'average_match': 33.33333333333333}"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_preds = (\n",
    "    [\n",
    "        \"Тр+идцять р+оків т+ому.\",\n",
    "        \"Тр+идцять р+оків том+у.\",\n",
    "        \"Тридцять р+оків том+у.\",\n",
    "        \"Тридц+ять рок+ів том+у.\",  # 0, cause all wrong stresses\n",
    "        \"Тр+идцять р+оків !\", # 0, cause wrong words\n",
    "        \"Тр+идцять р+оків\", # 0, cause wrong words\n",
    "    ],\n",
    "    [\n",
    "        \"Тр+идцять р+оків т+ому.\",\n",
    "        \"Тр+идцять р+оків т+ому.\",\n",
    "        \"Тр+идцять р+оків т+ому.\",\n",
    "        \"Тр+идцять р+оків т+ому.\",\n",
    "        \"Тр+идцять р+оків т+ому.\",\n",
    "        \"Тр+идцять р+оків т+ому.\",\n",
    "    ]\n",
    ")\n",
    "for pred, label in zip(eval_preds[0], eval_preds[1]):\n",
    "    encoded_pred = tokenizer(pred, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    encoded_label = tokenizer(label, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    result = compute_stress_metrics((encoded_pred, encoded_label))\n",
    "    print(result)\n",
    "    \n",
    "\n",
    "print(\"Average: \")\n",
    "encoded_preds = tokenizer(eval_preds[0], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "encoded_labels = tokenizer(eval_preds[1], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "result = compute_stress_metrics((encoded_preds, encoded_labels))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5b596-6eda-4967-9f5f-c544c2079a9f",
   "metadata": {},
   "source": [
    "# Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b32c4137-7358-45bf-829c-b74ef6b649b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pad_token_id = -100\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Hugging Face repository id\n",
    "repository_id = f\"{model_id.split('/')[1]}-accentor-model\"\n",
    "\n",
    "# Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=repository_id,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    \n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=2,\n",
    "    # max_steps=5000,\n",
    "    \n",
    "    logging_dir=f\"{repository_id}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=500,\n",
    "\n",
    "    load_best_model_at_end=False,\n",
    "    # metric_for_best_model=\"overall_f1\",\n",
    "    # push to hub parameters\n",
    "    # report_to=\"tensorboard\",\n",
    "\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_model_id=repository_id,\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"].select(range(500)),\n",
    "    compute_metrics=compute_stress_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6d71d-99d5-43c2-9c38-9c0de230c1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='374' max='135708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   374/135708 03:43 < 22:33:04, 1.67 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f9d5c989-c4d3-44a8-ac59-5ff89bf37bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015b850-3b91-4f7c-8c76-0773cd3ccdd0",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d1810-d6ca-4d8f-80c6-20db2b2c26b2",
   "metadata": {},
   "source": [
    "## Evaluate custom text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b567b3e-0bdb-4a94-82a9-aac6e5e7d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(input_text, model, tokenizer, device='cuda'):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=1500)\n",
    "\n",
    "    decoded_preds = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33c70ab3-82ee-4303-977b-19d90c292e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Прив+іт, +як спр+ави?'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Привіт, як справи?\"\n",
    "\n",
    "decoded_prediction = generate_prediction(input_text, model, tokenizer)\n",
    "decoded_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2faae1e3-937b-4685-a3d6-4f7e52da2165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'відкр+ити кл+ючем з+амок'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"відкрити ключем замок\"\n",
    "\n",
    "decoded_prediction = generate_prediction(input_text, model, tokenizer)\n",
    "decoded_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c78a40a7-0ff2-4f01-85a5-ceaa6f952e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' С+идить б+обер н+а б+ерезі'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Сидить бобер на березі\"\n",
    "\n",
    "decoded_prediction = generate_prediction(input_text, model, tokenizer)\n",
    "decoded_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d277b76f-2f12-4373-bed3-96fe96861a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Лист+я н+а б+ерезі'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Листя на березі\"\n",
    "\n",
    "decoded_prediction = generate_prediction(input_text, model, tokenizer)\n",
    "decoded_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9d4f6-cfa6-416e-a653-345fdbf38eaa",
   "metadata": {},
   "source": [
    "# Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "da29beac-1c48-4016-8cca-b02a5d76743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 99/1371 [00:42<09:02,  2.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "model.eval()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "all_predictions = []\n",
    "i = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(tokenized_dataset[\"eval\"]):\n",
    "        i += 1\n",
    "        if i==100: break\n",
    "\n",
    "        input_ids = torch.tensor(batch[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor(batch[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=1500)\n",
    "        decoded_preds = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        labels = batch[\"label\"]\n",
    "        labels = [l for l in labels if l != -100]\n",
    "        decoded_labels = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "        decoded_inputs = tokenizer.decode(batch[\"input_ids\"], skip_special_tokens=True)\n",
    "        \n",
    "        all_predictions.append((decoded_inputs, decoded_preds, decoded_labels))\n",
    "\n",
    "df_predictions = pd.DataFrame(all_predictions, columns=[\"Input Text\", \"Predictions\", \"True Labels\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "366b30fd-69a1-495c-abbe-f16b0477167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Text</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>True Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Тридцять років тому.</td>\n",
       "      <td>Тр+идцять р+оків <span style=\"color: red; font-weight: bold;\">том+у.</span></td>\n",
       "      <td>Тр+идцять р+оків <span style=\"color: red; font-weight: bold;\">т+ому.</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Потім було музичне училище по класу ударних інструментів.</td>\n",
       "      <td>П+отім бул+о муз+ичне уч+илище п+о кл+асу уд+арних інструм+ентів.</td>\n",
       "      <td>П+отім бул+о муз+ичне уч+илище п+о кл+асу уд+арних інструм+ентів.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>але вона може займатися.</td>\n",
       "      <td>ал+е вон+а м+оже займ+атися.</td>\n",
       "      <td>ал+е вон+а м+оже займ+атися.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Цю операцію потрібно провести якомога швидше. На цьому наголошують як українська влада, так і місцеві мешканці, яким вдалося виїхати з Маріуполя.</td>\n",
       "      <td>Ц+ю опер+ацію потр+ібно провест+и яком+ога шв+идше. Н+а цьом+у нагол+ошують +як укра+їнська вл+ада, т+ак +і місц+еві м+ешканці, як+им вдал+ося в+иїхати з Марі+уполя.</td>\n",
       "      <td>Ц+ю опер+ацію потр+ібно провест+и яком+ога шв+идше. Н+а цьом+у нагол+ошують +як укра+їнська вл+ада, т+ак +і місц+еві м+ешканці, як+им вдал+ося в+иїхати з Марі+уполя.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Тим часом Литва наказала своїм урядовцям позбутися телефонів к Сайомі та Хуавей.</td>\n",
       "      <td>Т+им ч+асом Литв+а наказ+ала <span style=\"color: red; font-weight: bold;\">сво+їм</span> уряд+овцям позб+утися телеф+онів к Сай+омі т+а Хуав+ей.</td>\n",
       "      <td>Т+им ч+асом Литв+а наказ+ала <span style=\"color: red; font-weight: bold;\">св+оїм</span> уряд+овцям позб+утися телеф+онів к Сай+омі т+а Хуав+ей.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>пів години обговорював перспективи вступу україни в нато з президентом сша джо Байденом</td>\n",
       "      <td>п+ів год+ини обгов+орював перспект+иви вст+упу укра+їни в н+ато з презид+ентом сш+а дж+о <span style=\"color: red; font-weight: bold;\">Байд+еном</span></td>\n",
       "      <td>п+ів год+ини обгов+орював перспект+иви вст+упу укра+їни в н+ато з презид+ентом сш+а дж+о <span style=\"color: red; font-weight: bold;\">Б+айденом</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>І також традиційно виголосив промову, звернувся.</td>\n",
       "      <td>+І т+акож традиц+ійно <span style=\"color: red; font-weight: bold;\">виголос+ив</span> пром+ову, зверн+увся.</td>\n",
       "      <td>+І т+акож традиц+ійно <span style=\"color: red; font-weight: bold;\">в+иголосив</span> пром+ову, зверн+увся.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>пояснює, що це все однозначно спричинене людською діяльністю.</td>\n",
       "      <td>по+яснює, щ+о ц+е вс+е однозн+ачно сприч+инене л+юдською <span style=\"color: red; font-weight: bold;\">д+іяльністю.</span></td>\n",
       "      <td>по+яснює, щ+о ц+е вс+е однозн+ачно сприч+инене л+юдською <span style=\"color: red; font-weight: bold;\">ді+яльністю.</span></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>І Джо Байден пообіцяв, що всі, хто хоче виїхати, виїдуть.</td>\n",
       "      <td>+І Дж+о <span style=\"color: red; font-weight: bold;\">Байд+ен</span> пообіц+яв, щ+о вс+і, хт+о х+оче в+иїхати, в+иїдуть.</td>\n",
       "      <td>+І Дж+о <span style=\"color: red; font-weight: bold;\">Б+айден</span> пообіц+яв, щ+о вс+і, хт+о х+оче в+иїхати, в+иїдуть.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Юрій Мамон, Євгенія Дюло, волос Америки, Каліфорнія.</td>\n",
       "      <td>+Юрій Мам+он, Євг+енія <span style=\"color: red; font-weight: bold;\">Д+юло,</span> в+олос Ам+ерики, Каліф+орнія.</td>\n",
       "      <td>+Юрій Мам+он, Євг+енія <span style=\"color: red; font-weight: bold;\">Дюл+о,</span> в+олос Ам+ерики, Каліф+орнія.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def highlight_differences(prediction, label):\n",
    "    pred_words = re.split(r'(\\s+|-)', prediction)\n",
    "    label_words = re.split(r'(\\s+|-)', label)\n",
    "\n",
    "    highlighted_pred = []\n",
    "    highlighted_label = []\n",
    "\n",
    "    for pred_word, label_word in zip(pred_words, label_words):\n",
    "        if pred_word != label_word:\n",
    "            highlighted_label.append(f'<span style=\"color: red; font-weight: bold;\">{label_word}</span>')\n",
    "            highlighted_pred.append(f'<span style=\"color: red; font-weight: bold;\">{pred_word}</span>')\n",
    "        else:\n",
    "            highlighted_label.append(label_word)\n",
    "            highlighted_pred.append(pred_word)\n",
    "\n",
    "    # Join the lists back into strings\n",
    "    highlighted_pred = ''.join(highlighted_pred)\n",
    "    highlighted_label = ''.join(highlighted_label)\n",
    "    return highlighted_pred, highlighted_label\n",
    "\n",
    "highlighted_preds = []\n",
    "highlighted_labels = []\n",
    "df_pred_highlighted = df_predictions.copy()\n",
    "\n",
    "for pred, label in zip(df_pred_highlighted[\"Predictions\"], df_pred_highlighted[\"True Labels\"]):\n",
    "    highlighted_pred, highlighted_label = highlight_differences(pred, label)\n",
    "    highlighted_preds.append(highlighted_pred)\n",
    "    highlighted_labels.append(highlighted_label)\n",
    "\n",
    "df_pred_highlighted[\"Predictions\"] = highlighted_preds\n",
    "df_pred_highlighted[\"True Labels\"] = highlighted_labels\n",
    "\n",
    "display(HTML(df_pred_highlighted.head(10).to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5bba9-c137-489a-b02e-dd442cf5a39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
